{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the functions, libraries and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.libraries import *\n",
    "from utils.objects import *\n",
    "from utils.functions import *\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date varialbes\n",
    "years = 20\n",
    "period=\"W\"\n",
    "cutoff_date=datetime.today().strftime('%Y-%m-%d')\n",
    "Ymd_str = datetime.today().strftime('%Y%m%d')\n",
    "Ym_str = datetime.today().strftime('%Y%m')\n",
    "Y_str = datetime.today().strftime('%Y')\n",
    "timeframe = 365*years\n",
    "today = datetime.today()\n",
    "end = today.strftime(\"%Y-%m-%d\")\n",
    "start = (today - dt.timedelta(days=timeframe)).strftime(\"%Y-%m-%d\")\n",
    "periods = period\n",
    "\n",
    "## Paths varialbes\n",
    "ROOT_PATH = Path(ROOT_PATH)\n",
    "DATA_FOLDER = r\"data\\result\\processed_data\"\n",
    "PREDICT_FOLDER = r\"data\\result\\prediction_data\"\n",
    "data_path = ROOT_PATH.joinpath(DATA_FOLDER, Y_str, Ym_str, Ymd_str)\n",
    "predict_path = ROOT_PATH.joinpath(PREDICT_FOLDER, Y_str, Ym_str, Ymd_str)\n",
    "## Config varialbes\n",
    "config_paths = [\n",
    "    \"config\\Market_Data_Config.csv\",\n",
    "    \"config\\Economic_Data_Config.csv\",\n",
    "    \"config\\Calc_Data_Config.csv\"]\n",
    "market_config = ROOT_PATH.joinpath(config_paths[0])\n",
    "economic_config = ROOT_PATH.joinpath(config_paths[1]).abspath()\n",
    "calc_config = ROOT_PATH.joinpath(config_paths[2]).abspath()\n",
    "target_list = pd.read_csv(market_config, sep=\";\", header=0).loc[:, \"Codigo\"].to_list()\n",
    "markets_used = pd.read_csv(market_config, sep=\";\", header=0).loc[:, \"Codigo\"].to_list()\n",
    "markets_remove = pd.read_csv(market_config, sep=\";\", header=0).loc[:, \"Market\"].to_list()\n",
    "# markets_used = ['SPY', 'GDX', 'BND']\n",
    "target = \"FTNT\"\n",
    "seed = 2  # ML random seed\n",
    "extract = True\n",
    "cross_val=5\n",
    "medidas = [\"mean\", \"median\", \"mode\", \"Min\", \"Percentil_25\", \"Percentil_75\",\"Max\", \"var\", \"std\", \"skew\", \"kurt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to explain os.mkdir() method\n",
    "\n",
    "# importing os module\n",
    "import os\n",
    "\n",
    "# Directory\n",
    "directory = \"GeeksforGeeks\"\n",
    "\n",
    "# Parent Directory path\n",
    "parent_dir = \"C:/Users/Joan Oliver/Documents/GitHub/Economic_Market_Forecasting/Economic_Market_Forecasting/EMF_webapp/src/data/result\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "# 'GeeksForGeeks' in\n",
    "# '/home / User / Documents'\n",
    "os.mkdir(path)\n",
    "# print(\"Directory '% s' created\" % directory)\n",
    "\n",
    "# # Directory\n",
    "# directory = \"Geeks\"\n",
    "\n",
    "# # Parent Directory path\n",
    "# parent_dir = \"D:/Pycharm projects\"\n",
    "\n",
    "# # mode\n",
    "# mode = 0o666\n",
    "\n",
    "# # Path\n",
    "# path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# # Create the directory\n",
    "# # 'GeeksForGeeks' in\n",
    "# # '/home / User / Documents'\n",
    "# # with mode 0o666\n",
    "# os.mkdir(path, mode)\n",
    "# print(\"Directory '% s' created\" % directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the US economic and market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Economic Data Analyzer class\n",
    "eda = EconomicDataAnalyzer()\n",
    "# Load the Market Data\n",
    "print(\"> Load the market data config\")\n",
    "market = eda.read_config(market_config)\n",
    "market_dict = eda.convert_to_dictionary(markets_used=markets_used)\n",
    "market_dict = market_dict['Market']\n",
    "# Load the economic data config\n",
    "print(\"> Load the economic data config\")\n",
    "econ = eda.read_config(economic_config)\n",
    "fred_series_dict = eda.convert_to_dictionary(markets_used=None)\n",
    "fred_series_dict = fred_series_dict[\"Indicador\"]\n",
    "calc = eda.read_config(calc_config)\n",
    "series_calc_dict = eda.convert_to_dictionary(markets_used=None)\n",
    "series_calc_dict = series_calc_dict[\"Indicador\"]\n",
    "# Setting up the indicator dictionaries\n",
    "print(\"> Setting up the indicator dictionaries\")\n",
    "indicators = {}\n",
    "for ind in list(econ[\"Tipo\"].unique()):\n",
    "    indicators[ind] = econ[econ[\"Tipo\"] == ind][\"Indicador\"].to_list()\n",
    "if extract == True:\n",
    "    # Extracting the indicator data\n",
    "    print(\"> Extracting the indicator data\")\n",
    "    indicators_df = eda.indicator_extraction(fred_series_dict, series_calc_dict, root_path=ROOT_PATH)\n",
    "    # Extracting the market data\n",
    "    print(\"> Extracting the market data\")\n",
    "    stocks = list(market_dict.keys())\n",
    "    market_df = eda.market_extraction(stocks, start, end, root_path=ROOT_PATH)\n",
    "else:\n",
    "    print(\"No data extraction, reading data from data file\")\n",
    "    path = ROOT_PATH.joinpath('data', 'raw', 'indicators_df.csv')\n",
    "    indicators_df = pd.read_csv(path)\n",
    "    path = ROOT_PATH.joinpath('data', 'raw', 'market_df.csv')\n",
    "    market_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the EU economic and market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Extracting European economic data\n",
    "# import eurostat\n",
    "# #### EU Yield Curve\n",
    "# yield_curve = eurostat.get_data_df(\"irt_euryld_d\")\n",
    "# yld_crv = yield_curve.loc[(~yield_curve[\"bonds\"].str.contains(\"AAA\")) & (yield_curve[\"yld_curv\"].str.contains(\"INS_FWD\")) & (~yield_curve[\"maturity\"].str.contains(\"M\")),:]\n",
    "# yld_crv.set_index(\"maturity\", inplace=True)\n",
    "# yld_crv[\"maturity_int\"] = yld_crv.index.str.split(\"Y\").str[1].astype(int)\n",
    "# yld_crv.sort_values(\"maturity_int\",ascending=True, inplace=True)\n",
    "# yld_crv.drop([\"maturity_int\"], axis=1, inplace=True)\n",
    "# yld_crv = yld_crv.T\n",
    "# yld_crv = yld_crv.loc[yld_crv.index.drop([\"freq\",'yld_curv', 'bonds', 'geo\\TIME_PERIOD'])]\n",
    "# yld_crv.index = pd.to_datetime(yld_crv.index)\n",
    "# yld_lst = []\n",
    "# for yld in yld_crv.columns:\n",
    "#     yld_lst.append(f\"EU_yield_{yld}\")\n",
    "# yld_crv.set_axis(yld_lst, axis=1, inplace=True)\n",
    "# intr_inds = pd.merge(indicators_df, yld_crv, left_index=True, right_index=True)\n",
    "# #### EU HICP\n",
    "# hicp = eurostat.get_data_df(\"PRC_HICP_MIDX\")\n",
    "# hicp = hicp.loc[(hicp[\"geo\\TIME_PERIOD\"] == \"EU\") & (hicp[\"coicop\"] == \"CP00\") & (hicp[\"unit\"] == \"I15\"),:].T\n",
    "# hicp.drop([\"freq\", \"geo\\TIME_PERIOD\", \"coicop\", \"unit\"], axis=0, inplace=True)\n",
    "# hicp.set_axis([\"EU CPI\"], axis=1, inplace=True)\n",
    "# hicp.index = pd.to_datetime(hicp.index)\n",
    "# intr_inds = pd.merge(intr_inds, hicp, left_index=True, right_index=True)\n",
    "# #### EU Government Deficit\n",
    "# gov_def = eurostat.get_data_df(\"GOV_10DD_EDPT1\")\n",
    "# gov_def = gov_def.loc[(gov_def[\"geo\\TIME_PERIOD\"] == \"EA20\") & (gov_def[\"sector\"] == \"S13\") & (gov_def[\"na_item\"] == \"B9\") & (gov_def[\"unit\"] == \"PC_GDP\"), :].T\n",
    "# gov_def.set_axis(gov_def.loc[\"geo\\TIME_PERIOD\"].values, axis=1, inplace=True)\n",
    "# gov_def.drop([\"freq\", \"geo\\TIME_PERIOD\", \"sector\", \"na_item\", \"unit\"], axis=0, inplace=True)\n",
    "# gov_def.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the indicator data\n",
    "print(\"> Cleaning the indicator data\")\n",
    "df_indicators, df_indicators_cum, df_indicators_diff, df_indicators_rets, df_indicators_limpio = eda.limpiar_indicators(\n",
    "    df_indicators=indicators_df, \n",
    "    indicator_dict=indicators, \n",
    "    resample=periods, \n",
    "    fill_method=\"ffill\", \n",
    "    start=start, \n",
    "    end=end, \n",
    "    root_path=ROOT_PATH)\n",
    "# Cleaning the market data\n",
    "print(\"> Cleaning market data\")\n",
    "df_market, df_markets_rets, df_markets_cum, df_markets_diff  = eda.limpiar_markets(\n",
    "    markets_dict=market_dict,\n",
    "    df_markets=market_df,\n",
    "    resample=periods, \n",
    "    fill_method=\"ffill\", \n",
    "    start=start, \n",
    "    end=end, \n",
    "    root_path=ROOT_PATH)\n",
    "### Merge indicator and market data\n",
    "list_market_dfs = [df_market,df_markets_rets,df_markets_cum,df_markets_diff]\n",
    "list_indicators_dfs = [df_indicators_limpio,df_indicators_rets,df_indicators_cum,df_indicators_diff]\n",
    "\n",
    "df_all_data, df_all_data_rets, df_all_data_cum, df_all_data_diff = eda.merge_data(list_market_dfs, list_indicators_dfs, root_path=ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Outliers \n",
    "df = eda.remove_outliers(df_all_data_rets)\n",
    "### Adding features\n",
    "df_all_data[\"CAPE Ratio\"] = df_all_data[\"SP500\"]/(df_all_data[\"Corporate Profits\"]*0.01)\n",
    "df[\"CAPE Ratio\"] = df_all_data[\"SP500\"]/(df_all_data[\"Corporate Profits\"]*0.01)\n",
    "\n",
    "# df_all_data[\"CAPE Ratio\"].plot()\n",
    "def trend_line(df, name, deg=2):\n",
    "    coef = np.polyfit(range(0,len(df[name])), df[name], deg)\n",
    "    x_trend = np.linspace(0,len(df[name]),len(df[name]))\n",
    "    y_trend = np.polyval(coef, x_trend)\n",
    "    df = pd.DataFrame(y_trend, index=df.index, columns=[name])\n",
    "    return df\n",
    "\n",
    "# df_all_data = pd.DataFrame()\n",
    "df_all_data[\"SP_GDP\"] = df_all_data[\"SP500\"]/(df_all_data[\"GDP\"]*.01)\n",
    "df_all_data[\"SP_GDP_trend\"] = trend_line(df_all_data, \"SP_GDP\", deg=5)\n",
    "df[\"SP_GDP\"] = df_all_data[\"SP500\"]/(df_all_data[\"GDP\"]*.01)\n",
    "df[\"SP_GDP_trend\"] = trend_line(df_all_data, \"SP_GDP\", deg=5)\n",
    "\n",
    "# df_all_data = pd.DataFrame()\n",
    "std = df_all_data[\"SP_GDP\"].std()\n",
    "df_all_data[\"SP_GDP_1std\"] = df_all_data[\"SP_GDP_trend\"] + (std)\n",
    "df[\"SP_GDP_1std\"] = df_all_data[\"SP_GDP_trend\"] + (std)\n",
    "\n",
    "# df_all_data = df_all_data.copy()\n",
    "# df_ts = df_all_data.loc[:,df_all_data.columns.str.contains(f\"t-\")]\n",
    "# df_all_data.drop(df_ts.columns,axis=1,inplace=True)\n",
    "for ma in df_all_data.columns:\n",
    "    df_all_data[f\"{ma}_MA\"] = df_all_data[[ma]].rolling(window=52).mean().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df_all_data[f\"{ma}_std\"] = df_all_data[[ma]].rolling(window=52).std().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df_all_data[f\"{ma}_trend\"] = trend_line(df_all_data[[ma]], ma, deg=6)\n",
    "    df_all_data[f\"{ma}_MA_trend_dif\"] = df_all_data[f\"{ma}_trend\"] - df_all_data[f\"{ma}_MA\"]\n",
    "    \n",
    "    df[f\"{ma}_MA\"] = df_all_data[[ma]].rolling(window=52).mean().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df[f\"{ma}_trend\"] = trend_line(df_all_data[[ma]], ma, deg=6)\n",
    "    df[f\"{ma}_MA_trend_dif\"] = df_all_data[f\"{ma}_trend\"] - df_all_data[f\"{ma}_MA\"]\n",
    "### Creating lags in the data\n",
    "list_data_dfs = [df_all_data,df_all_data_rets,df_all_data_cum,df_all_data_diff]\n",
    "\n",
    "df_all_lag_data, df_all_lag_data_rets, df_all_lag_data_cum, df_all_lag_data_diff = eda.lag_data(list_data_dfs, target, n_lags=24)\n",
    "df = eda.remove_outliers(df_all_lag_data_rets)\n",
    "for mkt in markets_remove:\n",
    "    if mkt == target:\n",
    "        pass\n",
    "    else:\n",
    "        for df_col in df.columns:\n",
    "            if mkt in df_col:\n",
    "                try:\n",
    "                    df.drop(df_col, axis=1, inplace=True)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "econ_ml = Preprocessor()\n",
    "### Feature Reduction\n",
    "#### Feature selection by correlation\n",
    "df_feat_corr = pd.DataFrame(df.corr().loc[target,:].sort_values(ascending=False))\n",
    "df_feat_relevant_corr = df_feat_corr[(df_feat_corr[target]>0.05) | (df_feat_corr[target]<-0.05)]\n",
    "df_feat_relevant_corr\n",
    "#### Indentifying the most important features\n",
    "##### Splitting the data\n",
    "\n",
    "##### Creating the baseline for feature importance\n",
    "baseline_models = econ_ml.define_baseline_models()\n",
    "\n",
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df, target_col=target, test_size=0.15)\n",
    "model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)\n",
    "\n",
    "print(\"> Performing feature importance analysis\")\n",
    "df_top_data, feature_importance, top_feature_importance, score = econ_ml.feature_importance(target=target, \n",
    "                                                                                                df_data=df.loc[:cutoff_date],\n",
    "                                                                                                model=best_model,\n",
    "                                                                                                accepted_importance=0.85)\n",
    "#### Feature removal\n",
    "def feature_removal(df, df_top_data, model_results, best_model_name, score):\n",
    "    best_model_score = model_results.loc[best_model_name,\"score\"]\n",
    "    if score > best_model_score*.9:\n",
    "        print(\"We choose to remove \"+str(len(df.columns)-len(df_top_data.columns))+\" features\")\n",
    "        df = df_top_data.copy()\n",
    "    else:\n",
    "        print(\"We choose to keep the original df with \"+str(len(df_top_data.columns))+\" features\")\n",
    "    return df\n",
    "\n",
    "df = feature_removal(df, df_top_data, model_results, best_model_name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the processed data (ready for ML)\n",
    "\n",
    "df.to_csv(data_path+f\"\\processed_data_{target}_{today_str}.csv\", index=True, index_label=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the processed data\n",
    "read_data = True\n",
    "\n",
    "if read_data:\n",
    "    # df1 = pd.read_csv(data_path+f\"\\processed_data_{today_str}.csv\", index_col=\"Date\")\n",
    "    df = pd.read_csv(r\"C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\data\\result\\processed_data\\processed_data_FTNT_20230812.csv\", \n",
    "                    index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "econ_ml = Preprocessor()\n",
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df, target_col=target, test_size=0.15)\n",
    "model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the grids\n",
    "params_RandomForest = {\n",
    "    \"n_estimators\": [120],\n",
    "    \"max_depth\": [10,15,17],\n",
    "    \"max_features\": [\"sqrt\", 3, 4]                          \n",
    "    }\n",
    "\n",
    "params_GradientBoosting = {\n",
    "    'n_estimators': [100, 150],  # 50, \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'max_depth': [5, 7],  \n",
    "    }\n",
    "\n",
    "params_XGBRegressor = {\n",
    "    'n_estimators': [150, 250],  # 100\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'max_depth': [ 5, 7, 11],\n",
    "    # 'subsample': [0.8, 1.0],\n",
    "    # 'max_leaf_nodes': [32, 64, 108]\n",
    "    }\n",
    "\n",
    "params_KNeighborsRegressor = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2],  \n",
    "    }\n",
    "\n",
    "params_SVR = {\n",
    "    'C': [0.1, 1.0, 10.0],  \n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  \n",
    "    'degree': [2, 3, 4],  \n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0],  \n",
    "    }\n",
    "\n",
    "baseline_params = {\n",
    "    \"RandomForest\":params_RandomForest,\n",
    "    \"GradientBoosting\":params_GradientBoosting,\n",
    "    \"SVR\":params_SVR,\n",
    "    \"KNeighborsRegressor\":params_KNeighborsRegressor,\n",
    "    \"XGBRegressor\":params_XGBRegressor\n",
    "}\n",
    "def model_gridSearch(baseline_models,baseline_params,model_results,X_train,y_train,X_test,y_test,cross_val=5):\n",
    "    y_test = y_test.copy()\n",
    "    models_gridsearch = {}\n",
    "    for name, model in baseline_models.items():\n",
    "        if name in model_results.index.values:\n",
    "            for mod,params in baseline_params.items():\n",
    "                if name == mod:\n",
    "                    models_gridsearch[mod] = GridSearchCV(model, params, cv=cross_val, scoring=\"neg_root_mean_squared_error\", verbose=1, n_jobs=1)\n",
    "                    models_gridsearch[mod].fit(X_train, y_train)\n",
    "    best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "    best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "    top_model = models_gridsearch[best_grids.loc[0,\"Grid\"]]\n",
    "    return models_gridsearch, best_grids, top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_gridsearch, best_grids, top_model = model_gridSearch(baseline_models,baseline_params,model_results,X_train,y_train,X_test,y_test,cross_val=cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(top_model, r'C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\model'+f\"\\{target}_best_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Indicator Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the stats\n",
    "mean = df.mean()\n",
    "var = df.var()\n",
    "drift = mean - (.5 * var)\n",
    "std = df.std()\n",
    "\n",
    "# Setting the Monte Carlo Varuables\n",
    "ind = 0\n",
    "T = 104\n",
    "num_ports = 100\n",
    "date_range = pd.date_range(start=cutoff_date, periods=T, freq=\"W\")\n",
    "\n",
    "dict_future = {}\n",
    "df_mean_future = pd.DataFrame(index=pd.date_range(start=cutoff_date, periods=T, freq=\"W\"))\n",
    "df_max_future = pd.DataFrame(index=pd.date_range(start=cutoff_date, periods=T, freq=\"W\"))\n",
    "df_min_future = pd.DataFrame(index=pd.date_range(start=cutoff_date, periods=T, freq=\"W\"))\n",
    "\n",
    "# Running through indicator columns to forecast\n",
    "for ind, col in enumerate(df.columns):\n",
    "    # Calculating the Weekly Returns\n",
    "    weekly_rets = np.exp(drift.values[ind] + std.values[ind] * norm.ppf(np.random.rand(T, num_ports)))\n",
    "\n",
    "    # Getting the most current weekly return (run it back if it's too small)\n",
    "    n = -1\n",
    "    S0 = 0\n",
    "    while (S0 < 0.01) and (S0 > -0.01):\n",
    "        S0 = df.cumsum().iloc[n,ind]\n",
    "        n = n - 1\n",
    "    # Creating the empty list and filling the first row\n",
    "    price_list = np.zeros_like(weekly_rets)\n",
    "    price_list[0] = S0\n",
    "\n",
    "    # Performing Monte Carlo Situlation a 'num_ports' number of times\n",
    "    for t in range(1,T):\n",
    "        price_list[t] = price_list[t-1] * weekly_rets[t]\n",
    "        dict_future[col] = pd.DataFrame(price_list,index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the forecast data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a list of forecasted futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Each item is a dataframe of a possible future of all indicators\n",
    "\n",
    "list_futures = []\n",
    "for n in range(0,num_ports):\n",
    "    globals()[\"df_future_\"+str(n)] = pd.DataFrame()\n",
    "    for indicator in dict_future.keys():\n",
    "        globals()[\"df_future_\"+str(n)][indicator] = dict_future[indicator][n]\n",
    "    list_futures.append(globals()[\"df_future_\"+str(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a custom merging function\n",
    "# def merge_columns_with_nans(row):\n",
    "#     merged_values = []\n",
    "#     for value in row:\n",
    "#         if pd.notna(value):\n",
    "#             merged_values.append(value)\n",
    "#     return merged_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging the forecasted data with the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "list_present_future = []\n",
    "for n,future in enumerate(list_futures):\n",
    "    globals()[\"df_future_\"+str(n)].index = pd.to_datetime(globals()[\"df_future_\"+str(n)].index)\n",
    "    df_presentVSfuture = pd.concat([df,globals()[\"df_future_\"+str(n)].pct_change()], axis=1, join=\"outer\")\n",
    "    globals()[\"df_present_future_\"+str(n)] = pd.DataFrame()\n",
    "    for col in df_presentVSfuture.columns:\n",
    "        # df1 = df_presentVSfuture[[col]].apply(merge_columns_with_nans, axis=1).apply(pd.Series)\n",
    "        # globals()[\"df_present_future_\"+str(n)][col] = df1\n",
    "        globals()[\"df_present_future_\"+str(n)][col] = df_presentVSfuture[col].fillna(0).iloc[:,0] + df_presentVSfuture[col].fillna(0).iloc[:,1]\n",
    "    list_present_future.append(globals()[\"df_present_future_\"+str(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_prediction(models_gridsearch, best_grids, top_model, X_test, y_test):\n",
    "    y_pred = models_gridsearch[best_grids.loc[0,\"Grid\"]].predict(X_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[target+\"_Prediction\"],index=y_test.index)\n",
    "    y_pred.index, y_test.index = pd.to_datetime(y_test.index), pd.to_datetime(y_test.index)\n",
    "    model_pred = pd.concat([y_test, y_pred], axis=1)\n",
    "    return y_pred, y_test, model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df[[target]].cumsum()\n",
    "list_prediction = [df_prediction]\n",
    "for n, present_future in enumerate(list_present_future):\n",
    "    df_pred = present_future.fillna(method=\"ffill\")\n",
    "\n",
    "    test_size = T/len(df_pred)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df_pred, target_col=target, test_size=test_size)\n",
    "    # model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)\n",
    "    y_pred = top_model.predict(X_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[target+\"_Prediction\"],index=y_test.index)\n",
    "    y_pred.index, y_test.index = pd.to_datetime(y_test.index), pd.to_datetime(y_test.index)\n",
    "    model_pred = pd.concat([y_test, y_pred], axis=1)\n",
    "    model_pred.columns = [target+\"_\"+str(n),target+\"_Prediction\"+\"_\"+str(n)]\n",
    "    latest_actual = df[[target]].cumsum().loc[df.index[-1]].values[0]\n",
    "    model_pred = model_pred.cumsum()+latest_actual\n",
    "    # df_prediction[target+\"_\"+str(n)] = model_pred[target+\"_\"+str(n)]\n",
    "    # df_prediction[target+\"_Prediction_\"+str(n)] = model_pred[target+\"_Prediction_\"+str(n)]\n",
    "    # df_prediction = pd.concat([df[target].cumsum(),model_pred.cumsum()+latest_actual], axis=1)\n",
    "    list_prediction.append(model_pred)\n",
    "df_all_predictions = pd.concat(list_prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_predictions.to_csv(predict_path+f\"\\prediction_{target}_{today_str}.csv\", index=True, index_label=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_predictions.plot(figsize=(20,8), legend=False, grid=True)\n",
    "# df_all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future_preds = (df_all_predictions.loc[:,df_all_predictions.columns.str.contains(\"Prediction\")]-latest_actual).dropna()\n",
    "for n in np.linspace(0,1,5):\n",
    "    print(round(n,2))\n",
    "    df_future_preds.quantile(round(n,2),axis=1).plot(figsize=(20,8), legend=False, grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_predictions.loc[:,df_all_predictions.columns.str.contains(\"_\")].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.conco((df_all_predictions.loc[:,df_all_predictions.columns.str.contains(\"Prediction\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\data\\result\\prediction_data\\prediction_SP500_20230812.csv\", \n",
    "                    index_col=\"Date\")\n",
    "df2 = pd.read_csv(r\"C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\data\\result\\prediction_data\\prediction_Gold_20230812.csv\", \n",
    "                    index_col=\"Date\")\n",
    "df3 = pd.read_csv(r\"C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\data\\result\\prediction_data\\prediction_CorporateBonds_20230812.csv\", \n",
    "                    index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future_preds = (df1.loc[:,df1.columns.str.contains(\"Prediction\")]-latest_actual).dropna()\n",
    "for n in np.linspace(0,1,5):\n",
    "    print(round(n,2))\n",
    "    df_future_preds.quantile(round(n,2),axis=1).plot(figsize=(20,8), legend=False, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for present_future in list_present_future:\n",
    "    test_size = T/len(present_future)\n",
    "    X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=present_future, target_col=target, test_size=test_size)\n",
    "    model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_future_mean.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performing Machine Learning\n",
    "### Pick the best model\n",
    "\n",
    "test_size = T/len(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df, target_col=target, test_size=test_size)\n",
    "model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the grids\n",
    "params_RandomForest = {\n",
    "    \"n_estimators\": [120],\n",
    "    \"max_depth\": [10,15,17],\n",
    "    \"max_features\": [\"sqrt\", 3, 4]                          \n",
    "    }\n",
    "\n",
    "params_GradientBoosting = {\n",
    "    'n_estimators': [50, 100, 150],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'max_depth': [3, 5, 7],  \n",
    "    }\n",
    "\n",
    "params_XGBRegressor = {\n",
    "    'n_estimators': [100, 150, 250],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'max_depth': [ 5, 7, 11],\n",
    "    # 'subsample': [0.8, 1.0],\n",
    "    # 'max_leaf_nodes': [32, 64, 108]\n",
    "    }\n",
    "\n",
    "params_KNeighborsRegressor = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2],  \n",
    "    }\n",
    "\n",
    "params_SVR = {\n",
    "    'C': [0.1, 1.0, 10.0],  \n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  \n",
    "    'degree': [2, 3, 4],  \n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0],  \n",
    "    }\n",
    "\n",
    "baseline_params = {\n",
    "    \"RandomForest\":params_RandomForest,\n",
    "    \"GradientBoosting\":params_GradientBoosting,\n",
    "    \"SVR\":params_SVR,\n",
    "    \"KNeighborsRegressor\":params_KNeighborsRegressor,\n",
    "    \"XGBRegressor\":params_XGBRegressor\n",
    "}\n",
    "def model_gridSearch(baseline_models,baseline_params,model_results,X_train,y_train,X_test,y_test,cross_val=5):\n",
    "    y_test = y_test.copy()\n",
    "    models_gridsearch = {}\n",
    "    for name, model in baseline_models.items():\n",
    "        if name in model_results.index.values:\n",
    "            for mod,params in baseline_params.items():\n",
    "                if name == mod:\n",
    "                    models_gridsearch[mod] = GridSearchCV(model, params, cv=cross_val, scoring=\"neg_root_mean_squared_error\", verbose=1, n_jobs=1)\n",
    "                    models_gridsearch[mod].fit(X_train, y_train)\n",
    "    best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "    best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "    top_model = models_gridsearch[best_grids.loc[0,\"Grid\"]]\n",
    "    return models_gridsearch, best_grids, top_model\n",
    "\n",
    "def best_prediction(models_gridsearch, best_grids, top_model, X_test, y_test):\n",
    "    y_pred = models_gridsearch[best_grids.loc[0,\"Grid\"]].predict(X_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[target+\"_Prediction\"],index=y_test.index)\n",
    "    y_pred.index, y_test.index = pd.to_datetime(y_test.index), pd.to_datetime(y_test.index)\n",
    "    model_pred = pd.concat([y_test, y_pred], axis=1)\n",
    "    return y_pred, y_test, model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"> Performing Machine Learning\")\n",
    "print(\">>> Performing Grid Search\")\n",
    "models_gridsearch, best_grids, top_model = model_gridSearch(baseline_models,baseline_params,model_results,X_train,y_train,X_test,y_test,cross_val=cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_future.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Performing Prediction\")\n",
    "for indicator in dict_future.keys():\n",
    "    y_pred, y_test, model_pred = best_prediction(models_gridsearch, best_grids, top_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 50th percentile of the monte carlo simulations\n",
    "dict_future[\"SP500\"].reindex(dict_future[\"SP500\"].iloc[-1].sort_values(ascending=False).index, axis=1).iloc[:,int(num_ports/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_test.index = pd.to_datetime(X_test.index)\n",
    "except:\n",
    "    pass\n",
    "full_test = pd.concat([model_pred, X_test], axis=1)\n",
    "print(\">>> Saving the best model and the data\")\n",
    "# Save the best model\n",
    "dump(top_model, r'C:\\Users\\Joan Oliver\\Documents\\GitHub\\Economic_Market_Forecasting\\Economic_Market_Forecasting\\EMF_webapp\\src\\model'+f\"\\{target}_best_model.joblib\")\n",
    "# Save the data\n",
    "model_pred.to_csv(data_path+f\"\\{target}_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_actual = df[[target]].cumsum().loc[model_pred.index[0]].values[0]\n",
    "pd.concat([df[target].cumsum(),model_pred.cumsum()+latest_actual], axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = df_future_max.fillna(method=\"ffill\")\n",
    "## Performing Machine Learning\n",
    "### Pick the best model\n",
    "\n",
    "test_size = T/len(df_max)\n",
    "\n",
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df_max, target_col=target, test_size=test_size)\n",
    "model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models_gridsearch[best_grids.loc[0,\"Grid\"]].predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=[target+\"_Prediction\"],index=y_test.index)\n",
    "y_pred.index, y_test.index = pd.to_datetime(y_test.index), pd.to_datetime(y_test.index)\n",
    "model_pred_max = pd.concat([y_test, y_pred], axis=1)\n",
    "latest_actual = df[[target]].cumsum().loc[model_pred_max.index[0]].values[0]\n",
    "df_max_prediction = pd.concat([df[target].cumsum(),model_pred_max.cumsum()+latest_actual], axis=1)\n",
    "df_max_prediction.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min = df_future_min.fillna(method=\"ffill\")\n",
    "\n",
    "test_size = T/len(df_min)\n",
    "\n",
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df_min, target_col=target, test_size=test_size)\n",
    "model_results, baseline_preds, best_model, best_model_name = econ_ml.baseline_ml(target, X_train, X_test, y_train, y_test, baseline_models)\n",
    "y_pred = models_gridsearch[best_grids.loc[0,\"Grid\"]].predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=[target+\"_Prediction\"],index=y_test.index)\n",
    "y_pred.index, y_test.index = pd.to_datetime(y_test.index), pd.to_datetime(y_test.index)\n",
    "model_pred_min = pd.concat([y_test, y_pred], axis=1)\n",
    "latest_actual = df[[target]].cumsum().loc[model_pred_min.index[0]].values[0]\n",
    "\n",
    "df_min_prediction = pd.concat([df[target].cumsum(),model_pred_min.cumsum()+latest_actual], axis=1)\n",
    "df_min_prediction.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
