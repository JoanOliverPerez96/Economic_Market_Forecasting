{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.libraries import *\n",
    "from utils.objects import *\n",
    "from utils.functions import *\n",
    "from definitions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting & Preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utiliza un rango de 20 años para la extraccion de datos económicos\n",
    "timeframe = 365*20\n",
    "today = datetime.today()\n",
    "end = today.strftime(\"%Y-%m-%d\")\n",
    "start = (today - dt.timedelta(days=timeframe)).strftime(\"%Y-%m-%d\")\n",
    "periods = \"W\"\n",
    "\n",
    "medidas = [\"mean\", \"median\", \"mode\", \"Min\", \"Percentil_25\", \"Percentil_75\",\"Max\", \"var\", \"std\", \"skew\", \"kurt\"]\n",
    "\n",
    "ROOT_PATH = Path(ROOT_PATH)\n",
    "config_paths = [\n",
    "    \"config\\Market_Data_Config.csv\",\n",
    "    \"config\\Economic_Data_Config.csv\",\n",
    "    \"config\\Calc_Data_Config.csv\"\n",
    "]\n",
    "market_config = ROOT_PATH.joinpath(config_paths[0])\n",
    "economic_config = ROOT_PATH.joinpath(config_paths[1]).abspath()\n",
    "calc_config = ROOT_PATH.joinpath(config_paths[2]).abspath()\n",
    "\n",
    "\n",
    "markets_used = ['SPY', 'GDX', 'BND']\n",
    "\n",
    "target = \"SP500\"\n",
    "\n",
    "# ML random seed\n",
    "seed = 2\n",
    "\n",
    "extract = True\n",
    "\n",
    "cutoff_date = \"2023-01-01\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Economic Data Analyzer class\n",
    "eda = EconomicDataAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Load the market data config\n",
      "> Load the economic data config\n",
      "> Setting up the indicator dictionaries\n",
      "> Extracting the indicator data\n",
      "   -->Extracting 10-Year Treasury Yield...\n",
      "   -->Extracting 2-Year Treasury Yield...\n",
      "   -->Extracting Federal Funds Effective Rate...\n",
      "   -->Extracting 1-Month Treasury Yield...\n",
      "   -->Extracting 3-Month Treasury Yield...\n",
      "   -->Extracting 5-Year Treasury Yield...\n",
      "   -->Extracting 20-Year Treasury Yield...\n",
      "   -->Extracting 30-Year Treasury Yield...\n",
      "   -->Extracting 15-Year Mortgage Rate...\n",
      "   -->Extracting 30-Year Mortgage Rate...\n",
      "   -->Extracting Unemployment Rate...\n",
      "   -->Extracting GDP...\n",
      "   -->Extracting Real GDP...\n",
      "   -->Extracting CPI...\n",
      "   -->Extracting PPI...\n",
      "   -->Extracting Consumer Confidence Index...\n",
      "   -->Extracting Government Debt...\n",
      "   -->Extracting Debt Service vs Disposable Income...\n",
      "   -->Extracting US Dollar Index...\n",
      "   -->Extracting Corporate Profits...\n",
      "   -->Extracting EBIT...\n",
      "   -->Extracting Retained Earnings...\n",
      "   -->Extracting Building Permits...\n",
      "   -->Extracting Velocity of Money...\n",
      "   -->Extracting Retail Sales...\n",
      "   -->Extracting Industrial Production...\n",
      "Indicators Extracted: Index(['10-Year Treasury Yield', '2-Year Treasury Yield',\n",
      "       'Federal Funds Effective Rate', '1-Month Treasury Yield',\n",
      "       '3-Month Treasury Yield', '5-Year Treasury Yield',\n",
      "       '20-Year Treasury Yield', '30-Year Treasury Yield',\n",
      "       '15-Year Mortgage Rate', '30-Year Mortgage Rate', 'Unemployment Rate',\n",
      "       'GDP', 'Real GDP', 'CPI', 'PPI', 'Consumer Confidence Index',\n",
      "       'Government Debt', 'Government Debt to GDP',\n",
      "       'Debt Service vs Disposable Income', 'US Dollar Index',\n",
      "       'Corporate Profits', 'EBIT', 'Retained Earnings', 'Building Permits',\n",
      "       'Velocity of Money', 'Retail Sales', 'Industrial Production', '3m5y',\n",
      "       '3m10y', '2y10y', '2y20y', '5y10y', '10y30y', '10yTrea30yFRM'],\n",
      "      dtype='object')\n",
      "> Extracting the market data\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "# Load the Market Data\n",
    "print(\"> Load the market data config\")\n",
    "market = eda.read_config(market_config)\n",
    "market_dict = eda.convert_to_dictionary(markets_used=markets_used)\n",
    "market_dict = market_dict['Market']\n",
    "market_dict\n",
    "# Load the economic data config\n",
    "print(\"> Load the economic data config\")\n",
    "econ = eda.read_config(economic_config)\n",
    "fred_series_dict = eda.convert_to_dictionary(markets_used=None)\n",
    "fred_series_dict = fred_series_dict[\"Indicador\"]\n",
    "calc = eda.read_config(calc_config)\n",
    "series_calc_dict = eda.convert_to_dictionary(markets_used=None)\n",
    "series_calc_dict = series_calc_dict[\"Indicador\"]\n",
    "econ\n",
    "# Setting up the indicator dictionaries\n",
    "print(\"> Setting up the indicator dictionaries\")\n",
    "indicators = {}\n",
    "for ind in list(econ[\"Tipo\"].unique()):\n",
    "    indicators[ind] = econ[econ[\"Tipo\"] == ind][\"Indicador\"].to_list()\n",
    "if extract == True:\n",
    "    # Extracting the indicator data\n",
    "    print(\"> Extracting the indicator data\")\n",
    "    indicators_df = eda.indicator_extraction(fred_series_dict, series_calc_dict, root_path=ROOT_PATH)\n",
    "    # Extracting the market data\n",
    "    print(\"> Extracting the market data\")\n",
    "    stocks = list(market_dict.keys())\n",
    "    market_df = eda.market_extraction(stocks, start, end, root_path=ROOT_PATH)\n",
    "else:\n",
    "    print(\"No data extraction, reading data from data file\")\n",
    "    path = ROOT_PATH.joinpath('data', 'raw', 'indicators_df.csv')\n",
    "    indicators_df = pd.read_csv(path)\n",
    "    path = ROOT_PATH.joinpath('data', 'raw', 'market_df.csv')\n",
    "    market_df = pd.read_csv(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cleaning the indicator data\n",
      "> Cleaning market data\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the indicator data\n",
    "print(\"> Cleaning the indicator data\")\n",
    "df_indicators, df_indicators_cum, df_indicators_diff, df_indicators_rets, df_indicators_limpio = eda.limpiar_indicators(\n",
    "    df_indicators=indicators_df, \n",
    "    indicator_dict=indicators, \n",
    "    resample=periods, \n",
    "    fill_method=\"ffill\", \n",
    "    start=start, \n",
    "    end=end, \n",
    "    root_path=ROOT_PATH)\n",
    "# Cleaning the market data\n",
    "print(\"> Cleaning market data\")\n",
    "df_market, df_markets_rets, df_markets_cum, df_markets_diff  = eda.limpiar_markets(\n",
    "    markets_dict=market_dict,\n",
    "    df_markets=market_df,\n",
    "    resample=periods, \n",
    "    fill_method=\"ffill\", \n",
    "    start=start, \n",
    "    end=end, \n",
    "    root_path=ROOT_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge indicator and market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_market_dfs = [df_market,df_markets_rets,df_markets_cum,df_markets_diff]\n",
    "list_indicators_dfs = [df_indicators_limpio,df_indicators_rets,df_indicators_cum,df_indicators_diff]\n",
    "\n",
    "df_all_data, df_all_data_rets, df_all_data_cum, df_all_data_diff = eda.merge_data(list_market_dfs, list_indicators_dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data[\"CAPE Ratio\"] = df_all_data[\"SP500\"]/(df_all_data[\"Corporate Profits\"]*0.01)\n",
    "# df_all_data[\"CAPE Ratio\"].plot()\n",
    "def trend_line(df, name, deg=2):\n",
    "    coef = np.polyfit(range(0,len(df[name])), df[name], deg)\n",
    "    x_trend = np.linspace(0,len(df[name]),len(df[name]))\n",
    "    y_trend = np.polyval(coef, x_trend)\n",
    "    df = pd.DataFrame(y_trend, index=df.index, columns=[name])\n",
    "    return df \n",
    "# df_all_data = pd.DataFrame()\n",
    "df_all_data[\"SP_GDP\"] = df_all_data[\"SP500\"]/(df_all_data[\"GDP\"]*.01)\n",
    "df_all_data[\"SP_GDP_trend\"] = trend_line(df_all_data, \"SP_GDP\", deg=5)\n",
    "# df_all_data = pd.DataFrame()\n",
    "std = df_all_data[\"SP_GDP\"].std()\n",
    "df_all_data[\"SP_GDP_1std\"] = df_all_data[\"SP_GDP_trend\"] + (std)\n",
    "\n",
    "# df_all_data = df_all_data.copy()\n",
    "# df_ts = df_all_data.loc[:,df_all_data.columns.str.contains(f\"t-\")]\n",
    "# df_all_data.drop(df_ts.columns,axis=1,inplace=True)\n",
    "for ma in df_all_data.columns:\n",
    "    df_all_data[f\"{ma}_MA\"] = df_all_data[[ma]].rolling(window=52).mean().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df_all_data[f\"{ma}_trend\"] = trend_line(df_all_data[[ma]], ma, deg=6)\n",
    "    df_all_data[f\"{ma}_MA_trend_dif\"] = df_all_data[f\"{ma}_trend\"] - df_all_data[f\"{ma}_MA\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add_feat = df_all_data.loc[:,df_all_data.columns.str.contains(\"_\")]\n",
    "# for feat in df_add_feat.columns:\n",
    "#     df_all_data_rets[feat] = df_add_feat[feat].pct_change().fillna(method=\"ffill\").fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_data, df_MKT_GDP = eda.mkt_gdp(df_all_data, \"MKT_GDP\", n_stds=2, deg=2)\n",
    "# df_MKT_GDP.plot()\n",
    "# df_MKT_GDP.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating lags in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_dfs = [df_all_data,df_all_data_rets,df_all_data_cum,df_all_data_diff]\n",
    "\n",
    "df_all_lag_data, df_all_lag_data_rets, df_all_lag_data_cum, df_all_lag_data_diff = eda.lag_data(list_data_dfs, target, n_lags=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 469)\n"
     ]
    }
   ],
   "source": [
    "df = eda.remove_outliers(df_all_lag_data_rets)\n",
    "print(df.shape)\n",
    "# df = pd.concat([df,df_MKT_GDP], axis=1).dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_ml = Preprocessor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indentifying the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"> Performing feature importance analysis\")\n",
    "df_top_data, rf_feature_importance, rf_top_feature_importance, score = econ_ml.rf_feature_importance(target=target, df_data=df.loc[:cutoff_date], accepted_importance=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = econ_ml.train_test_split_data(data=df, target_col=target, test_size=0.15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale, X_test_scale = econ_ml.scaler(X_train=X_train, X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerasbeats import prep_time_series, NBeatsModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(epochs,batch_size,validation_split):\n",
    "    # LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=64, return_sequences=True, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    lstm_model.add(LSTM(32))\n",
    "    lstm_model.add(Dense(units=1))\n",
    "    lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    lstm_model.fit(X_train_scale, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    return lstm_model\n",
    "\n",
    "def gru_model(epochs,batch_size,validation_split):\n",
    "    # gru model\n",
    "    gru_model = Sequential()\n",
    "    gru_model.add(GRU(units=64, return_sequences=True, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    gru_model.add(GRU(32))\n",
    "    gru_model.add(Dense(units=1))\n",
    "    gru_model.compile(optimizer='adam', loss='mse')\n",
    "    gru_model.fit(X_train_scale, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    return gru_model\n",
    "\n",
    "def cnn_model(epochs,batch_size,validation_split):\n",
    "    # cnn model\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(units=1))\n",
    "    cnn_model.compile(optimizer='adam', loss='mse')\n",
    "    cnn_model.fit(X_train_scale, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    return cnn_model\n",
    "def n_beats():\n",
    "    n_beats = NBeatsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LSTM\":lstm_model(epochs=15,batch_size=16,validation_split=0.2),\n",
    "    \"GRU\":gru_model(epochs=15,batch_size=16,validation_split=0.2),\n",
    "    \"CNN\":cnn_model(epochs=15,batch_size=16,validation_split=0.2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Evaluate the model: {name}: {model.evaluate(X_test_scale, y_test)}\")\n",
    "    # model.evaluate(X_test_scale, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Review of model: {name}\")\n",
    "    # if name != \"CNN\":\n",
    "    try:\n",
    "        df_history = pd.DataFrame({\"Loss\":model.history.history['loss'], \"Val_Loss\":model.history.history['val_loss']})\n",
    "        df_history.plot(figsize=(10, 6), title=name)\n",
    "    except:\n",
    "        pass\n",
    "    pred = model.predict(X_test_scale)\n",
    "    predictions = pd.DataFrame(pred, index=y_test.index)\n",
    "    predictions.index = pd.to_datetime(predictions.index)\n",
    "    model.save(f'models\\models\\{name}_best_model_no_rets.h5')\n",
    "    model.save_weights(f'models\\weights\\{name}_best_model_no_rets.h5') \n",
    "    # print(model.summary())\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"{name} -> {X_test.index[0]} - {X_test.index[-1]}\")\n",
    "    # plt.plot(df_all_lag_data[\"SP500\"].pct_change())\n",
    "    plt.plot(pd.to_datetime(y_test.index), y_test.cumsum(), label='Actual')\n",
    "    plt.plot((predictions.cumsum()), label=\"Pred\")\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
